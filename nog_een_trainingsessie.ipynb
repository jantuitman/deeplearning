{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1thycLsNCi3NJGJx5bIFjYMi8Ta-iMKTL",
      "authorship_tag": "ABX9TyPrX5QBRxI26fl90UPPN1mj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jantuitman/deeplearning/blob/main/nog_een_trainingsessie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31Ftqt6-uL2r",
        "outputId": "e29cec15-b3d6-40bf-bbf0-baf82bd1d5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers;\n",
        "!pip install torch;\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  T5ForConditionalGeneration, Adafactor, T5Tokenizer, AutoTokenizer, PreTrainedModel\n",
        "import torch\n",
        "import time\n",
        "\n",
        "import json"
      ],
      "metadata": {
        "id": "3aYcxu9fumE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read a json file containing an array of objects\n",
        "# each object has a \"question\" and \"answer\" field\n",
        "# the question is the input and the answer is the output\n",
        "# we will return a list of tuples\n",
        "def read_dataset2(filename):\n",
        "    with open(filename) as f:\n",
        "        data = json.load(f)\n",
        "        # data has the following structure:\n",
        "        # data = {\n",
        "        #     \"paragraphs\": [\n",
        "        #         {\n",
        "        #             \"translated_context\": \"text\",\n",
        "        #             \"translated_question\": \"text\",\n",
        "        #             \"translated_answers\": [\n",
        "        #                 \"text\",\n",
        "        #                 \"text\",\n",
        "        #                 \"text\"\n",
        "        #             ],\n",
        "        #             \"is_impossible\": true\n",
        "        #         },\n",
        "        #         ...\n",
        "        #     ]\n",
        "        # }\n",
        "        # if is_impossible is true, then translated_answers is empty and we want to return the answer \"Onbekend\"\n",
        "        # if is_impossible is false, then translated_answers is not empty and we want to return the first answer\n",
        "        # we will return a list of tuples\n",
        "        # we need to make a question_text from the translated_context and translated_question\n",
        "        result= []\n",
        "        previous_data = []\n",
        "        previous_paragraph_text = ''\n",
        "        for paragraph in data['paragraphs']:\n",
        "            if previous_paragraph_text != paragraph['translated_context']:\n",
        "              # we have a new context.\n",
        "              # store previous context.\n",
        "              if len(previous_data) >0:\n",
        "                context_text = f\"Extraheer vragen van context: {paragraph['translated_context']}\"\n",
        "                result.append((context_text,'\\n\\n'.join(previous_data)))\n",
        "                previous_data = []\n",
        "            # parse current context into previous_data array.\n",
        "            previous_paragraph_text = paragraph['translated_context']    \n",
        "            if not paragraph['is_impossible']:\n",
        "              question_text = paragraph['translated_question']\n",
        "              previous_data.append( question_text)\n",
        "        return result"
      ],
      "metadata": {
        "id": "BKPfQftturNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read a json file containing an array of objects\n",
        "# each object has a \"question\" and \"answer\" field\n",
        "# the question is the input and the answer is the output\n",
        "# we will return a list of tuples\n",
        "def read_dataset(filename):\n",
        "    with open(filename) as f:\n",
        "        data = json.load(f)\n",
        "        # data has the following structure:\n",
        "        # data = {\n",
        "        #     \"paragraphs\": [\n",
        "        #         {\n",
        "        #             \"translated_context\": \"text\",\n",
        "        #             \"translated_question\": \"text\",\n",
        "        #             \"translated_answers\": [\n",
        "        #                 \"text\",\n",
        "        #                 \"text\",\n",
        "        #                 \"text\"\n",
        "        #             ],\n",
        "        #             \"is_impossible\": true\n",
        "        #         },\n",
        "        #         ...\n",
        "        #     ]\n",
        "        # }\n",
        "        # if is_impossible is true, then translated_answers is empty and we want to return the answer \"Onbekend\"\n",
        "        # if is_impossible is false, then translated_answers is not empty and we want to return the first answer\n",
        "        # we will return a list of tuples\n",
        "        # we need to make a question_text from the translated_context and translated_question\n",
        "        result= []\n",
        "        for paragraph in data['paragraphs']:\n",
        "            question_text = f\"Context: {paragraph['translated_context']}\\n\\nVraag: {paragraph['translated_question']}\"\n",
        "            answer_text = \"Onbekend\"\n",
        "            if not paragraph['is_impossible']:\n",
        "               answer_text = paragraph['translated_answers'][0]\n",
        "            result.append((question_text, answer_text))\n",
        "        return result"
      ],
      "metadata": {
        "id": "Ekh9BFrZBli1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "def my_collator(data):\n",
        "    # Tokenize the input and output sequences\n",
        "    input_text = [example[0] for example in data]\n",
        "    target_text = [example[1] for example in data]\n",
        "    input_ids = tokenizer.batch_encode_plus(input_text, return_tensors='pt', pad_to_max_length=True)['input_ids']\n",
        "    attention_mask = tokenizer.batch_encode_plus(input_text, return_tensors='pt', pad_to_max_length=True)['attention_mask']\n",
        "    labels = tokenizer.batch_encode_plus(target_text, return_tensors='pt', pad_to_max_length=True)['input_ids']\n",
        "\n",
        "    return { \"input_ids\":torch.stack([input_id for input_id in input_ids]), \n",
        "            \"attention_mask\": torch.stack([a for a in attention_mask]),\n",
        "            \"labels\": torch.stack([l for l in labels])}"
      ],
      "metadata": {
        "id": "WtNSPisow5Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "model_name='yhavinga/t5-base-dutch'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo_1SuQYx0Jr",
        "outputId": "39b947a5-1045-4f2c-c664-8ad28d10a646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_dataset('drive/MyDrive/InputData/dev-nl.json')\n",
        "train_data, val_data = train_test_split(data,test_size=0.1)\n",
        "print(train_data[0])\n",
        "print(train_data[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4l99Map0vfA",
        "outputId": "63864446-66ca-41ad-805a-681fb213829f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Context: In zijn uitvoerig verslag schreef Céloron: \"Alles wat ik kan zeggen is dat de inboorlingen van deze plaatsen zeer slecht staan tegenover de Fransen, en zich geheel aan de Engelsen wijden. Ik weet niet hoe ze teruggebracht kunnen worden.\" Nog voor zijn terugkeer naar Montreal kwamen er berichten over de situatie in de Ohio Country naar Londen en Parijs, waarbij elke partij voorstelde actie te ondernemen. William Shirley, de expansionistische gouverneur van de provincie Massachusetts Bay, was bijzonder krachtig en verklaarde dat de Britse kolonisten niet veilig zouden zijn zolang de Fransen aanwezig waren. De conflicten tussen de koloniën, die werden uitgevochten door plunderende partijen met Indiaanse bondgenoten, hadden al tientallen jaren geduurd, wat leidde tot een levendige handel in Europese koloniale gevangenen van beide kanten.\\n\\nVraag: Wat vond Celeron van de relaties met de Indianen?', 'zeer slecht gezind tegenover de Fransen, en zijn volledig toegewijd aan de Engelsen')\n",
            "('Context: De atletische rivaliteit tussen Harvard en Yale is intens in elke sport waarin ze elkaar ontmoeten, en bereikt elk najaar een hoogtepunt in de jaarlijkse voetbalontmoeting, die teruggaat tot 1875 en meestal gewoon \"The Game\" wordt genoemd. Hoewel het footballteam van Harvard niet langer een van de beste van het land is, zoals een eeuw geleden in de begindagen van het football (het won de Rose Bowl in 1920), hebben zowel Harvard als Yale de manier waarop het spel wordt gespeeld beïnvloed. In 1903 introduceerde Harvard Stadium een nieuw tijdperk in het voetbal met het allereerste permanente stadion van gewapend beton in zijn soort in het land. De structuur van het stadion speelde een rol in de evolutie van het universiteitsspel. Om het alarmerende aantal doden en zwaargewonden in de sport te verminderen, stelde Walter Camp (voormalig aanvoerder van het Yale footballteam) voor het veld te verbreden om het spel opener te maken. Maar het stadion was te smal voor een breder speelveld. Dus moesten er andere stappen worden genomen. Camp zou in plaats daarvan revolutionaire nieuwe regels steunen voor het seizoen 1906. Deze omvatten het legaliseren van de voorwaartse pass, misschien wel de belangrijkste regelwijziging in de geschiedenis van de sport.\\n\\nVraag: In welk jaar werd Harvard Stadium het eerste met beton versterkte stadion in het land?', '1903')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.optimization import AdafactorSchedule\n",
        "adafactor = Adafactor(\n",
        "    model.parameters(),\n",
        "    lr=1e-3,\n",
        "    eps=(1e-30, 1e-3),\n",
        "    clip_threshold=1.0,\n",
        "    decay_rate=-0.8,\n",
        "    beta1=None,\n",
        "    weight_decay=0.0,\n",
        "    relative_step=False,\n",
        "    scale_parameter=False,\n",
        "    warmup_init=False\n",
        ")\n",
        "# adafactor = Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3,clip_threshold=1.0)\n",
        "lr_scheduler = AdafactorSchedule(adafactor)"
      ],
      "metadata": {
        "id": "OB7EHEsd1uHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# main output dir\n",
        "main_output_dir = './drive/MyDrive/OutputModels/t5_new5'\n",
        "# Configure the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f'{main_output_dir}/results',\n",
        "    evaluation_strategy = \"steps\",\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    learning_rate=1e-3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=20,\n",
        "    logging_dir=f'{main_output_dir}/logs',\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True\n",
        ")"
      ],
      "metadata": {
        "id": "X1GQ7nZHx6N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    data_collator=my_collator,\n",
        "    \n",
        ")\n",
        "\n",
        "# Run the fine-tuning\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aKl9dU5zx_6k",
        "outputId": "2ea68cbd-966f-4527-c4eb-be778e2fb16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 10685\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 26720\n",
            "  Number of trainable parameters = 222884352\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26720' max='26720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [26720/26720 1:52:31, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.733500</td>\n",
              "      <td>0.651710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.535800</td>\n",
              "      <td>0.626338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.409400</td>\n",
              "      <td>0.586958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.474900</td>\n",
              "      <td>0.593788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.392600</td>\n",
              "      <td>0.593623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.282100</td>\n",
              "      <td>0.628109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.233900</td>\n",
              "      <td>0.632640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.281100</td>\n",
              "      <td>0.580850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.223600</td>\n",
              "      <td>0.633897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.163000</td>\n",
              "      <td>0.677743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.125600</td>\n",
              "      <td>0.692887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.161800</td>\n",
              "      <td>0.667423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.683868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.093100</td>\n",
              "      <td>0.749809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.063200</td>\n",
              "      <td>0.857816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.083300</td>\n",
              "      <td>0.780549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.057100</td>\n",
              "      <td>0.865676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.046100</td>\n",
              "      <td>0.874916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.036600</td>\n",
              "      <td>0.839579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.038200</td>\n",
              "      <td>0.808432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.032500</td>\n",
              "      <td>0.854823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.024000</td>\n",
              "      <td>0.901823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.011900</td>\n",
              "      <td>0.950043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.018500</td>\n",
              "      <td>0.975514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.010100</td>\n",
              "      <td>0.995090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.006100</td>\n",
              "      <td>1.052456</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-1000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-1000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-1000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-1000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-2000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-2000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-2000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-2000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-3000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-3000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-3000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-3000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-4000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-4000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-4000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-4000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-5000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-5000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-5000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-5000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-6000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-6000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-6000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-6000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-7000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-7000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-7000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-7000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-8000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-8000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-8000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-8000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-9000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-9000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-9000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-9000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-10000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-10000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-10000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-10000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-11000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-11000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-11000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-11000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-12000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-12000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-12000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-12000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-13000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-13000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-13000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-13000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-14000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-14000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-14000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-14000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-15000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-15000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-15000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-15000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-16000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-16000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-16000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-16000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-17000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-17000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-17000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-17000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-18000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-18000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-18000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-18000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-19000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-19000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-19000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-19000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-20000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-20000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-20000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-20000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-21000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-21000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-21000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-21000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-22000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-22000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-22000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-22000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-23000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-23000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-23000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-23000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-24000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-24000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-24000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-24000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-25000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-25000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-25000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-25000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1188\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-26000\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-26000/config.json\n",
            "Configuration saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-26000/generation_config.json\n",
            "Model weights saved in ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-26000/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./drive/MyDrive/OutputModels/t5_new5/results/checkpoint-8000 (score: 0.5808501839637756).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=26720, training_loss=0.18720045637027055, metrics={'train_runtime': 6752.9436, 'train_samples_per_second': 31.645, 'train_steps_per_second': 3.957, 'total_flos': 1.023546532105728e+17, 'train_loss': 0.18720045637027055, 'epoch': 20.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}